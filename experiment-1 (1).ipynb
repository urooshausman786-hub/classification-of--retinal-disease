{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":4130910,"sourceType":"datasetVersion","datasetId":2440665}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport os\nimport time\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report, precision_recall_curve, average_precision_score\nfrom sklearn.preprocessing import label_binarize\nimport seaborn as sns\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications import EfficientNetB0, ResNet50, DenseNet121, MobileNet, MobileNetV2, VGG16\nfrom tensorflow.keras.optimizers import AdamW, Adam\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nimport math\nimport itertools\nimport cv2\n\n# Data loading\ndata_dir = r\"/kaggle/input/eye-diseases-classification/dataset\"\nfilepaths = []\nlabels = []\nfolds = os.listdir(data_dir)\nfor fold in folds:\n    foldpath = os.path.join(data_dir, fold)\n    filelist = os.listdir(foldpath)\n    for file in filelist:\n        fpath = os.path.join(foldpath, file)\n        filepaths.append(fpath)\n        labels.append(fold)\n\nFseries = pd.Series(filepaths, name='filepaths')\nLseries = pd.Series(labels, name='labels')\ndf = pd.concat([Fseries, Lseries], axis=1)\n\nprint(\"Class distribution:\")\nprint(df['labels'].value_counts())\nprint(f\"Number of unique classes: {df['labels'].nunique()}\")\n\n# Split data\ntrain_df, dummy_df = train_test_split(df, train_size=0.8, shuffle=True, random_state=42)\nvalid_df, test_df = train_test_split(dummy_df, train_size=0.6, shuffle=True, random_state=42)\n\nprint(f\"Train samples: {len(train_df)}\")\nprint(f\"Valid samples: {len(valid_df)}\")\nprint(f\"Test samples: {len(test_df)}\")\n\n# Create data generators\nbatch_size = 32\nimg_size = (224, 224)\nchannels = 3\n\ntr_gen = ImageDataGenerator(rescale=1./255)\nts_gen = ImageDataGenerator(rescale=1./255)\n\ntrain_gen = tr_gen.flow_from_dataframe(\n    train_df, x_col='filepaths', y_col='labels', \n    target_size=img_size, class_mode='categorical',\n    color_mode='rgb', shuffle=True, batch_size=batch_size\n)\nvalid_gen = ts_gen.flow_from_dataframe(\n    valid_df, x_col='filepaths', y_col='labels', \n    target_size=img_size, class_mode='categorical',\n    color_mode='rgb', shuffle=True, batch_size=batch_size\n)\ntest_gen = ts_gen.flow_from_dataframe(\n    test_df, x_col='filepaths', y_col='labels', \n    target_size=img_size, class_mode='categorical',\n    color_mode='rgb', shuffle=False, batch_size=batch_size\n)\n\n# Get number of classes\nnum_classes = 4\nclass_names = list(train_gen.class_indices.keys())\nprint(f\"Classes: {class_names}\")\n\n# Multiscale Attention Layer for Eye Disease Classifier\nclass MultiscaleAttentionLayer(layers.Layer):\n    def __init__(self, filters, **kwargs):\n        super(MultiscaleAttentionLayer, self).__init__(**kwargs)\n        self.filters = filters\n        \n    def build(self, input_shape):\n        # Channel attention\n        self.channel_attention = keras.Sequential([\n            layers.GlobalAveragePooling2D(),\n            layers.Dense(self.filters // 8, activation='relu'),\n            layers.Dense(self.filters, activation='sigmoid'),\n            layers.Reshape((1, 1, self.filters))\n        ])\n        \n        # Spatial attention\n        self.spatial_attention = keras.Sequential([\n            layers.Conv2D(1, (7, 7), padding='same', activation='sigmoid')\n        ])\n        \n        super(MultiscaleAttentionLayer, self).build(input_shape)\n    \n    def call(self, inputs):\n        # Channel attention\n        channel_att = self.channel_attention(inputs)\n        x = layers.Multiply()([inputs, channel_att])\n        \n        # Spatial attention\n        avg_pool = tf.reduce_mean(x, axis=-1, keepdims=True)\n        max_pool = tf.reduce_max(x, axis=-1, keepdims=True)\n        spatial_input = layers.Concatenate()([avg_pool, max_pool])\n        spatial_att = self.spatial_attention(spatial_input)\n        x = layers.Multiply()([x, spatial_att])\n        \n        return x\n    \n    def get_config(self):\n        config = super(MultiscaleAttentionLayer, self).get_config()\n        config.update({'filters': self.filters})\n        return config\n\n# Eye Disease Classifier\ndef create_eye_disease_classifier(num_classes):\n    \"\"\"\n    Create Eye Disease Classifier with VGG16 base and multiscale attention\n    \"\"\"\n    try:\n        # Load pre-trained VGG16\n        base_model = VGG16(\n            weights='imagenet',\n            include_top=False,\n            input_shape=(224, 224, 3)\n        )\n        \n        # Freeze early layers, unfreeze last few blocks for fine-tuning\n        for layer in base_model.layers[:-4]:\n            layer.trainable = False\n        for layer in base_model.layers[-4:]:\n            layer.trainable = True\n            \n        # Build the complete model\n        inputs = keras.Input(shape=(224, 224, 3), name='input_image')\n        \n        # VGG16 feature extraction\n        x = base_model(inputs, training=False)\n        \n        # Get the number of filters from VGG16 output\n        vgg_output_filters = x.shape[-1]\n        \n        # Multiscale attention mechanism\n        x = MultiscaleAttentionLayer(filters=vgg_output_filters, name='multiscale_attention')(x)\n        \n        # Global Average Pooling\n        x = layers.GlobalAveragePooling2D(name='global_avg_pool')(x)\n        \n        # Dropout for regularization\n        x = layers.Dropout(0.5, name='dropout_1')(x)\n        \n        # Dense layer with batch normalization\n        x = layers.Dense(256, activation='relu', name='dense_1')(x)\n        x = layers.BatchNormalization(name='batch_norm')(x)\n        x = layers.Dropout(0.25, name='dropout_2')(x)\n        \n        # Output layer with softmax activation\n        predictions = layers.Dense(num_classes, activation='softmax', name='predictions')(x)\n        \n        # Create the model\n        model = keras.Model(inputs, predictions, name='EyeDiseaseClassifier')\n        \n        print(f\"Eye Disease Classifier built successfully with {model.count_params()} parameters\")\n        return model\n        \n    except Exception as e:\n        print(f\"Error building model: {str(e)}\")\n        print(\"Building simplified model...\")\n        return create_eye_disease_classifier_simplified(num_classes)\n\ndef create_eye_disease_classifier_simplified(num_classes):\n    \"\"\"\n    Build a simplified model without custom layers as fallback\n    \"\"\"\n    # Load pre-trained VGG16\n    base_model = VGG16(\n        weights='imagenet',\n        include_top=False,\n        input_shape=(224, 224, 3)\n    )\n    \n    # Freeze early layers\n    for layer in base_model.layers[:-4]:\n        layer.trainable = False\n    for layer in base_model.layers[-4:]:\n        layer.trainable = True\n        \n    # Build simplified model\n    inputs = keras.Input(shape=(224, 224, 3))\n    x = base_model(inputs, training=False)\n    \n    # Simple attention using Global Average Pooling and Dense layers\n    gap = layers.GlobalAveragePooling2D()(x)\n    attention_weights = layers.Dense(x.shape[-1], activation='sigmoid')(gap)\n    attention_weights = layers.Reshape((1, 1, x.shape[-1]))(attention_weights)\n    x = layers.Multiply()([x, attention_weights])\n    \n    x = layers.GlobalAveragePooling2D()(x)\n    x = layers.Dropout(0.5)(x)\n    x = layers.Dense(256, activation='relu')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(0.25)(x)\n    predictions = layers.Dense(num_classes, activation='softmax')(x)\n    \n    model = keras.Model(inputs, predictions, name='EyeDiseaseClassifier_Simplified')\n    \n    print(f\"Simplified Eye Disease Classifier built successfully with {model.count_params()} parameters\")\n    return model\n\n# ViT Small Implementation\nclass PatchEmbedding(layers.Layer):\n    def __init__(self, patch_size=16, embed_dim=384):\n        super().__init__()\n        self.patch_size = patch_size\n        self.embed_dim = embed_dim\n        self.projection = layers.Conv2D(\n            embed_dim, kernel_size=patch_size, strides=patch_size, padding='valid'\n        )\n        \n    def call(self, x):\n        batch_size = tf.shape(x)[0]\n        patches = self.projection(x)\n        patches = tf.reshape(patches, [batch_size, -1, self.embed_dim])\n        return patches\n\nclass MultiHeadAttention(layers.Layer):\n    def __init__(self, embed_dim, num_heads):\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.head_dim = embed_dim // num_heads\n        \n        self.qkv = layers.Dense(embed_dim * 3)\n        self.projection = layers.Dense(embed_dim)\n        \n    def call(self, x):\n        batch_size, seq_len, embed_dim = tf.shape(x)[0], tf.shape(x)[1], tf.shape(x)[2]\n        \n        qkv = self.qkv(x)\n        qkv = tf.reshape(qkv, [batch_size, seq_len, 3, self.num_heads, self.head_dim])\n        qkv = tf.transpose(qkv, [2, 0, 3, 1, 4])\n        q, k, v = qkv[0], qkv[1], qkv[2]\n        \n        scale = tf.cast(self.head_dim, tf.float32) ** -0.5\n        attn = tf.matmul(q, k, transpose_b=True) * scale\n        attn = tf.nn.softmax(attn, axis=-1)\n        \n        out = tf.matmul(attn, v)\n        out = tf.transpose(out, [0, 2, 1, 3])\n        out = tf.reshape(out, [batch_size, seq_len, embed_dim])\n        \n        return self.projection(out)\n\nclass TransformerBlock(layers.Layer):\n    def __init__(self, embed_dim, num_heads, mlp_ratio=4.0, dropout=0.1):\n        super().__init__()\n        self.norm1 = layers.LayerNormalization(epsilon=1e-6)\n        self.attn = MultiHeadAttention(embed_dim, num_heads)\n        self.dropout1 = layers.Dropout(dropout)\n        \n        self.norm2 = layers.LayerNormalization(epsilon=1e-6)\n        mlp_hidden_dim = int(embed_dim * mlp_ratio)\n        self.mlp = keras.Sequential([\n            layers.Dense(mlp_hidden_dim, activation='gelu'),\n            layers.Dropout(dropout),\n            layers.Dense(embed_dim),\n            layers.Dropout(dropout)\n        ])\n        \n    def call(self, x, training=None):\n        attn_out = self.attn(self.norm1(x))\n        x = x + self.dropout1(attn_out, training=training)\n        \n        mlp_out = self.mlp(self.norm2(x), training=training)\n        x = x + mlp_out\n        \n        return x\n\nclass VisionTransformerSmall(keras.Model):\n    def __init__(self, \n                 img_size=224,\n                 patch_size=16, \n                 num_classes=4,\n                 embed_dim=384,\n                 depth=12,\n                 num_heads=6,\n                 mlp_ratio=4.0,\n                 dropout=0.1):\n        super().__init__()\n        \n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.num_patches = (img_size // patch_size) ** 2\n        \n        self.patch_embed = PatchEmbedding(patch_size, embed_dim)\n        \n        self.class_token = self.add_weight(\n            shape=(1, 1, embed_dim),\n            initializer='random_normal',\n            trainable=True,\n            name='class_token'\n        )\n        \n        self.pos_embed = self.add_weight(\n            shape=(1, self.num_patches + 1, embed_dim),\n            initializer='random_normal',\n            trainable=True,\n            name='pos_embed'\n        )\n        \n        self.dropout = layers.Dropout(dropout)\n        \n        self.blocks = [\n            TransformerBlock(embed_dim, num_heads, mlp_ratio, dropout)\n            for _ in range(depth)\n        ]\n        \n        self.norm = layers.LayerNormalization(epsilon=1e-6)\n        self.head = layers.Dense(num_classes)\n        \n    def call(self, x, training=None):\n        batch_size = tf.shape(x)[0]\n        \n        x = self.patch_embed(x)\n        \n        class_tokens = tf.broadcast_to(self.class_token, [batch_size, 1, tf.shape(self.class_token)[-1]])\n        x = tf.concat([class_tokens, x], axis=1)\n        \n        x = x + self.pos_embed\n        x = self.dropout(x, training=training)\n        \n        for block in self.blocks:\n            x = block(x, training=training)\n            \n        x = self.norm(x)\n        \n        class_token_final = x[:, 0]\n        return self.head(class_token_final)\n\ndef create_vit_small(num_classes):\n    return VisionTransformerSmall(\n        img_size=224,\n        patch_size=16,\n        num_classes=num_classes,\n        embed_dim=384,\n        depth=12,\n        num_heads=6,\n        mlp_ratio=4.0,\n        dropout=0.1\n    )\n\n# Other model creation functions\ndef create_efficientnet(num_classes):\n    base_model = EfficientNetB0(\n        weights='imagenet',\n        include_top=False,\n        input_shape=(224, 224, 3)\n    )\n    \n    model = keras.Sequential([\n        base_model,\n        layers.GlobalAveragePooling2D(),\n        layers.Dropout(0.2),\n        layers.Dense(num_classes, activation='softmax')\n    ])\n    \n    return model\n\ndef create_mobilenet_v2(num_classes):\n    base_model = MobileNetV2(\n        weights='imagenet',\n        include_top=False,\n        input_shape=(224, 224, 3)\n    )\n    \n    model = keras.Sequential([\n        base_model,\n        layers.GlobalAveragePooling2D(),\n        layers.Dropout(0.2),\n        layers.Dense(num_classes, activation='softmax')\n    ])\n    \n    return model\n\ndef create_vgg16(num_classes):\n    base_model = VGG16(\n        weights='imagenet',\n        include_top=False,\n        input_shape=(224, 224, 3)\n    )\n    \n    model = keras.Sequential([\n        base_model,\n        layers.GlobalAveragePooling2D(),\n        layers.Dropout(0.2),\n        layers.Dense(num_classes, activation='softmax')\n    ])\n    \n    return model\n\ndef create_denseNet201(num_classes):\n    base_model = DenseNet121(\n        weights='imagenet',\n        include_top=False,\n        input_shape=(224, 224, 3)\n    )\n    \n    model = keras.Sequential([\n        base_model,\n        layers.GlobalAveragePooling2D(),\n        layers.Dropout(0.2),\n        layers.Dense(num_classes, activation='softmax')\n    ])\n    \n    return model\n\ndef create_mobilenet_v2(num_classes):\n    base_model = MobileNetV2(\n        weights='imagenet',\n        include_top=False,\n        input_shape=(224, 224, 3)\n    )\n    \n    model = keras.Sequential([\n        base_model,\n        layers.GlobalAveragePooling2D(),\n        layers.Dropout(0.2),\n        layers.Dense(num_classes, activation='softmax')\n    ])\n    \n    return model\n\n# GradCAM implementation\ndef make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n    \"\"\"Generate GradCAM heatmap\"\"\"\n    try:\n        # Create a model that maps the input image to the activations of the last conv layer\n        grad_model = tf.keras.models.Model(\n            [model.inputs], [model.get_layer(last_conv_layer_name).output, model.output]\n        )\n        \n        # Compute the gradient of the top predicted class\n        with tf.GradientTape() as tape:\n            last_conv_layer_output, preds = grad_model(img_array)\n            if pred_index is None:\n                pred_index = tf.argmax(preds[0])\n            class_channel = preds[:, pred_index]\n        \n        # Gradient of the output neuron with regard to the output feature map\n        grads = tape.gradient(class_channel, last_conv_layer_output)\n        \n        # Mean intensity of the gradient over a specific feature map channel\n        pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n        \n        # Multiply each channel by \"how important this channel is\"\n        last_conv_layer_output = last_conv_layer_output[0]\n        heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n        heatmap = tf.squeeze(heatmap)\n        \n        # Normalize the heatmap between 0 & 1\n        heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n        return heatmap.numpy()\n    except Exception as e:\n        print(f\"GradCAM error: {e}\")\n        return None\n\n# Fixed GradCAM implementation with proper layer detection\n\ndef get_last_conv_layer_name(model, model_name):\n    \"\"\"Enhanced function to get the last convolutional layer name\"\"\"\n    conv_layers = []\n    \n    def collect_conv_layers(layer_or_model, prefix=\"\"):\n        \"\"\"Recursively collect all conv layer names\"\"\"\n        if hasattr(layer_or_model, 'layers'):\n            for layer in layer_or_model.layers:\n                layer_name = f\"{prefix}{layer.name}\" if prefix else layer.name\n                \n                # Check if it's a conv layer\n                if any(conv_type in layer.__class__.__name__.lower() for conv_type in ['conv2d', 'separableconv2d', 'depthwiseconv2d']):\n                    conv_layers.append(layer_name)\n                \n                # Recursively check sublayers\n                collect_conv_layers(layer, prefix=f\"{layer_name}/\")\n    \n    # Start collection from the model\n    collect_conv_layers(model)\n    \n    # If no conv layers found directly, try to access base model\n    if not conv_layers:\n        try:\n            # For Sequential models\n            if hasattr(model, 'layers') and len(model.layers) > 0:\n                first_layer = model.layers[0]\n                if hasattr(first_layer, 'layers'):\n                    collect_conv_layers(first_layer)\n        except:\n            pass\n    \n    # Print available conv layers for debugging\n    print(f\"Available conv layers in {model_name}: {conv_layers[-5:]}\")  # Show last 5\n    \n    # Return the last conv layer found\n    return conv_layers[-1] if conv_layers else None\n\ndef make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n    \"\"\"Enhanced GradCAM with better error handling\"\"\"\n    try:\n        # Handle nested layer names (e.g., \"base_model/block5_conv3\")\n        if '/' in last_conv_layer_name:\n            layer_parts = last_conv_layer_name.split('/')\n            current_layer = model\n            \n            # Navigate through nested layers\n            for part in layer_parts:\n                if hasattr(current_layer, 'get_layer'):\n                    current_layer = current_layer.get_layer(part)\n                elif hasattr(current_layer, 'layers'):\n                    # Find layer by name in layers list\n                    found = False\n                    for layer in current_layer.layers:\n                        if layer.name == part:\n                            current_layer = layer\n                            found = True\n                            break\n                    if not found:\n                        raise ValueError(f\"Layer {part} not found\")\n                else:\n                    raise ValueError(f\"Cannot navigate to layer {part}\")\n            \n            target_layer = current_layer\n        else:\n            # Simple case - direct layer access\n            target_layer = model.get_layer(last_conv_layer_name)\n        \n        # Create gradient model\n        grad_model = tf.keras.models.Model(\n            [model.inputs], \n            [target_layer.output, model.output]\n        )\n        \n        # Compute gradients\n        with tf.GradientTape() as tape:\n            last_conv_layer_output, preds = grad_model(img_array)\n            if pred_index is None:\n                pred_index = tf.argmax(preds[0])\n            class_channel = preds[:, pred_index]\n        \n        # Get gradients\n        grads = tape.gradient(class_channel, last_conv_layer_output)\n        \n        # Handle None gradients\n        if grads is None:\n            print(f\"Warning: No gradients computed for layer {last_conv_layer_name}\")\n            return None\n        \n        # Compute importance weights\n        pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n        \n        # Generate heatmap\n        last_conv_layer_output = last_conv_layer_output[0]\n        heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n        heatmap = tf.squeeze(heatmap)\n        \n        # Normalize heatmap\n        heatmap = tf.maximum(heatmap, 0) / (tf.math.reduce_max(heatmap) + 1e-10)\n        return heatmap.numpy()\n        \n    except Exception as e:\n        print(f\"GradCAM failed for {last_conv_layer_name}: {str(e)}\")\n        return None\n\ndef plot_gradcam_examples_fixed(model, test_gen, class_names, model_name, num_examples=4):\n    \"\"\"Fixed GradCAM plotting with fallback strategies\"\"\"\n    print(f\"\\nGenerating GradCAM for {model_name}...\")\n    \n    # Get last conv layer with enhanced detection\n    last_conv_layer = get_last_conv_layer_name(model, model_name)\n    \n    if not last_conv_layer:\n        print(f\"No convolutional layers found in {model_name} - skipping GradCAM\")\n        return\n    \n    print(f\"Using layer: {last_conv_layer}\")\n    \n    # Try alternative layers if the first one fails\n    alternative_layers = []\n    if 'vgg16' in last_conv_layer.lower():\n        alternative_layers = ['block5_conv3', 'block5_conv2', 'block4_conv3']\n    elif 'efficientnet' in model_name.lower():\n        alternative_layers = ['top_activation', 'top_conv', 'block7a_expand_activation']\n    elif 'mobilenet' in model_name.lower():\n        alternative_layers = ['out_relu', 'Conv_1', 'block_16_depthwise']\n    \n    successful_layer = None\n    test_heatmap = None\n    \n    # Test the primary layer\n    test_gen.reset()\n    test_images, test_labels = next(iter(test_gen))\n    test_img = np.expand_dims(test_images[0], axis=0)\n    \n    test_heatmap = make_gradcam_heatmap(test_img, model, last_conv_layer, pred_index=0)\n    if test_heatmap is not None:\n        successful_layer = last_conv_layer\n    else:\n        # Try alternative layers\n        for alt_layer in alternative_layers:\n            try:\n                test_heatmap = make_gradcam_heatmap(test_img, model, alt_layer, pred_index=0)\n                if test_heatmap is not None:\n                    successful_layer = alt_layer\n                    print(f\"Using alternative layer: {alt_layer}\")\n                    break\n            except:\n                continue\n    \n    if successful_layer is None:\n        print(f\"All GradCAM attempts failed for {model_name}\")\n        return\n    \n    # Plot GradCAM examples\n    plt.figure(figsize=(20, 10))\n    plt.suptitle(f'GradCAM Visualizations - {model_name} (Layer: {successful_layer})', \n                 fontsize=16, fontweight='bold')\n    \n    test_gen.reset()\n    images, labels = next(iter(test_gen))\n    \n    for i in range(min(num_examples, len(images))):\n        img_array = np.expand_dims(images[i], axis=0)\n        \n        # Make prediction\n        preds = model.predict(img_array, verbose=0)\n        pred_class = np.argmax(preds[0])\n        true_class = np.argmax(labels[i])\n        confidence = np.max(preds[0])\n        \n        # Generate heatmap\n        heatmap = make_gradcam_heatmap(img_array, model, successful_layer, pred_class)\n        \n        # Plot original\n        plt.subplot(2, num_examples, i + 1)\n        plt.imshow(images[i])\n        plt.title(f'Original\\nTrue: {class_names[true_class]}', fontsize=10)\n        plt.axis('off')\n        \n        # Plot GradCAM\n        plt.subplot(2, num_examples, i + 1 + num_examples)\n        plt.imshow(images[i])\n        if heatmap is not None:\n            # Resize heatmap to match image size\n            heatmap_resized = cv2.resize(heatmap, (224, 224))\n            plt.imshow(heatmap_resized, alpha=0.4, cmap='jet')\n            status = \"Success\"\n        else:\n            status = \"Failed\"\n        \n        plt.title(f'GradCAM ({status})\\nPred: {class_names[pred_class]}\\nConf: {confidence:.3f}', \n                 fontsize=10)\n        plt.axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n\ndef debug_model_structure(model, model_name):\n    \"\"\"Debug function to understand model structure\"\"\"\n    print(f\"\\n=== MODEL STRUCTURE DEBUG: {model_name} ===\")\n    \n    def print_layers(layer_or_model, indent=0):\n        prefix = \"  \" * indent\n        if hasattr(layer_or_model, 'layers'):\n            print(f\"{prefix}{layer_or_model.__class__.__name__}: {getattr(layer_or_model, 'name', 'unnamed')}\")\n            for i, layer in enumerate(layer_or_model.layers):\n                print(f\"{prefix}â”œâ”€ [{i}] {layer.__class__.__name__}: {layer.name}\")\n                if hasattr(layer, 'layers') and len(layer.layers) > 0:\n                    print_layers(layer, indent + 1)\n        else:\n            print(f\"{prefix}{layer_or_model.__class__.__name__}: {getattr(layer_or_model, 'name', 'unnamed')}\")\n    \n    print_layers(model)\n    print(\"=\" * 50)\n\n# Modified plotting function call in main training loop\ndef plot_gradcam_examples(model, test_gen, class_names, model_name, num_examples=4):\n    \"\"\"Wrapper function that includes debugging\"\"\"\n    # Debug model structure first\n    debug_model_structure(model, model_name)\n    \n    # Use the fixed GradCAM implementation\n    plot_gradcam_examples_fixed(model, test_gen, class_names, model_name, num_examples)\n\n# Additional helper function to manually specify conv layer names\ndef get_manual_conv_layer_mapping():\n    \"\"\"Manual mapping for problematic models\"\"\"\n    return {\n        'EfficientNet-B0': ['efficientnetb0/block6a_expand_conv', 'efficientnetb0/top_conv'],\n        'MobileNet-V2': ['mobilenetv2_1.00_224/block_16_depthwise', 'mobilenetv2_1.00_224/Conv_1'],\n        'VGG16': ['vgg16/block5_conv3', 'vgg16/block5_conv2'],\n        'DenseNet201': ['densenet201/conv5_block32_2_conv', 'densenet201/conv5_block32_concat'],\n        'Eye Disease Classifier': ['vgg16/block5_conv3', 'multiscale_attention']\n    }\n# Enhanced visualization functions\ndef plot_clean_training_history(history, model_name):\n    \"\"\"Plot clean training history with 4-digit precision\"\"\"\n    tr_acc = history.history['accuracy']\n    tr_loss = history.history['loss']\n    val_acc = history.history['val_accuracy']\n    val_loss = history.history['val_loss']\n    \n    # Find best epochs\n    index_loss = np.argmin(val_loss)\n    val_lowest = val_loss[index_loss]\n    index_acc = np.argmax(val_acc)\n    acc_highest = val_acc[index_acc]\n    \n    Epochs = [i+1 for i in range(len(tr_acc))]\n    loss_label = f'best epoch= {str(index_loss + 1)}'\n    acc_label = f'best epoch= {str(index_acc + 1)}'\n    \n    # Plot training history\n    plt.figure(figsize=(15, 8))\n    plt.style.use('fivethirtyeight')\n    \n    # Loss plot\n    plt.subplot(1, 2, 1)\n    plt.plot(Epochs, tr_loss, label='Training loss', linewidth=2)\n    plt.plot(Epochs, val_loss, label='Validation loss', linewidth=2)\n    plt.scatter(index_loss + 1, val_lowest, s=150, c='blue', label=loss_label, zorder=5)\n    plt.title(f'{model_name} - Training and Validation Loss', fontsize=14, fontweight='bold')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    \n    # Accuracy plot\n    plt.subplot(1, 2, 2)\n    plt.plot(Epochs, tr_acc, 'r', label='Training Accuracy', linewidth=2)\n    plt.plot(Epochs, val_acc, 'g', label='Validation Accuracy', linewidth=2)\n    plt.scatter(index_acc + 1, acc_highest, s=150, c='blue', label=acc_label, zorder=5)\n    plt.title(f'{model_name} - Training and Validation Accuracy', fontsize=14, fontweight='bold')\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Print best epoch information with 4-digit precision\n    print(f\"\\n{model_name} Training Summary:\")\n    print(f\"  Best Validation Loss: {val_lowest:.4f} at epoch {index_loss + 1}\")\n    print(f\"  Best Validation Accuracy: {acc_highest:.4f} at epoch {index_acc + 1}\")\n    print(f\"  Final Training Accuracy: {tr_acc[-1]:.4f}\")\n    print(f\"  Final Validation Accuracy: {val_acc[-1]:.4f}\")\n    print(f\"  Overfitting (Train-Val): {tr_acc[-1] - val_acc[-1]:.4f}\")\n\ndef plot_confusion_matrix(y_true, y_pred, class_names, model_name):\n    \"\"\"Plot confusion matrix with 4-digit precision\"\"\"\n    cm = confusion_matrix(y_true, y_pred)\n    \n    plt.figure(figsize=(10, 8))\n    plt.style.use('default')\n    \n    # Calculate percentages\n    cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n    \n    # Create combined annotations (count and percentage)\n    annotations = []\n    for i in range(cm.shape[0]):\n        row = []\n        for j in range(cm.shape[1]):\n            row.append(f'{cm[i,j]}\\n({cm_percent[i,j]:.4f})')\n        annotations.append(row)\n    \n    sns.heatmap(cm, annot=annotations, fmt='', cmap='Blues', \n                xticklabels=class_names, yticklabels=class_names,\n                cbar_kws={'label': 'Count'})\n    plt.title(f'Confusion Matrix - {model_name}\\n(Count and Proportion)', fontsize=14, fontweight='bold')\n    plt.xlabel('Predicted')\n    plt.ylabel('Actual')\n    plt.tight_layout()\n    plt.show()\n    \n    return cm\n\ndef generate_classification_report_4digits(y_true, y_pred, class_names, model_name):\n    \"\"\"Generate detailed classification report with 4-digit precision\"\"\"\n    report = classification_report(y_true, y_pred, target_names=class_names, \n                                 digits=4, output_dict=True)\n    \n    print(f\"\\n{'='*80}\")\n    print(f\"CLASSIFICATION REPORT - {model_name}\")\n    print(f\"{'='*80}\")\n    \n    # Print header\n    print(f\"{'Class':<15} {'Precision':<12} {'Recall':<12} {'F1-Score':<12} {'Support':<10}\")\n    print(\"-\" * 80)\n    \n    # Print per-class metrics\n    for class_name in class_names:\n        metrics = report[class_name]\n        print(f\"{class_name:<15} \"\n              f\"{metrics['precision']:<12.4f} \"\n              f\"{metrics['recall']:<12.4f} \"\n              f\"{metrics['f1-score']:<12.4f} \"\n              f\"{int(metrics['support']):<10}\")\n    \n    print(\"-\" * 80)\n    \n    # Print averages\n    for avg_type in ['macro avg', 'weighted avg']:\n        metrics = report[avg_type]\n        print(f\"{avg_type:<15} \"\n              f\"{metrics['precision']:<12.4f} \"\n              f\"{metrics['recall']:<12.4f} \"\n              f\"{metrics['f1-score']:<12.4f} \"\n              f\"{int(metrics['support']):<10}\")\n    \n    print(f\"\\nAccuracy: {report['accuracy']:.4f}\")\n    print(f\"{'='*80}\")\n    \n    return report\n\ndef compile_and_train_model(model, model_name, train_gen, valid_gen, epochs=40):\n    \"\"\"Compile and train model with fixed callbacks (no ReduceLROnPlateau)\"\"\"\n    print(f\"\\n{'='*50}\")\n    print(f\"Training {model_name}\")\n    print(f\"{'='*50}\")\n    \n    # Use fixed learning rate instead of schedule to avoid callback conflicts\n    if 'Eye Disease' in model_name or 'ViT' in model_name:\n        optimizer = AdamW(learning_rate=1e-4, weight_decay=1e-4)\n    else:\n        optimizer = keras.optimizers.Adam(learning_rate=1e-4)\n    \n    # Compile the model\n    model.compile(\n        optimizer=optimizer,\n        loss='categorical_crossentropy',\n        metrics=['accuracy', keras.metrics.TopKCategoricalAccuracy(k=2, name='top_2_accuracy')]\n    )\n    \n    # Simple callbacks without ReduceLROnPlateau to avoid conflicts\n    callbacks = [\n        EarlyStopping(\n            monitor='val_accuracy',\n            patience=10,  # Increased patience since we removed ReduceLROnPlateau\n            restore_best_weights=True,\n            verbose=1\n        )\n    ]\n    \n    # Train model\n    history = model.fit(\n        train_gen,\n        validation_data=valid_gen,\n        epochs=40,\n        verbose=1\n    )\n    \n    # Plot individual training history\n    plot_clean_training_history(history, model_name)\n    \n    return model, history\n\ndef measure_inference_time(model, test_gen, num_batches=10):\n    \"\"\"Measure inference time\"\"\"\n    times = []\n    \n    for i, (x_batch, y_batch) in enumerate(test_gen):\n        if i >= num_batches:\n            break\n        \n        start_time = time.time()\n        _ = model.predict(x_batch, verbose=0)\n        end_time = time.time()\n        \n        times.append(end_time - start_time)\n    \n    avg_time_per_batch = np.mean(times)\n    avg_time_per_image = avg_time_per_batch / batch_size\n    \n    return avg_time_per_batch, avg_time_per_image\n\ndef plot_combined_training_histories(histories):\n    \"\"\"Combined training histories visualization\"\"\"\n    plt.figure(figsize=(20, 12))\n    plt.style.use('fivethirtyeight')\n    \n    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FECA57', '#FF9FF3']\n    \n    # Training Loss\n    plt.subplot(2, 2, 1)\n    for idx, (model_name, history) in enumerate(histories.items()):\n        epochs = [i+1 for i in range(len(history.history['loss']))]\n        plt.plot(epochs, history.history['loss'], \n                color=colors[idx % len(colors)], linewidth=2, label=model_name)\n    plt.title('Training Loss Comparison', fontsize=16, fontweight='bold')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    \n    # Validation Loss\n    plt.subplot(2, 2, 2)\n    for idx, (model_name, history) in enumerate(histories.items()):\n        epochs = [i+1 for i in range(len(history.history['val_loss']))]\n        val_loss = history.history['val_loss']\n        plt.plot(epochs, val_loss, \n                color=colors[idx % len(colors)], linewidth=2, label=model_name)\n        \n        # Mark best epoch\n        best_epoch = np.argmin(val_loss) + 1\n        best_loss = min(val_loss)\n        plt.scatter(best_epoch, best_loss, \n                   color=colors[idx % len(colors)], s=100, zorder=5)\n        \n    plt.title('Validation Loss Comparison', fontsize=16, fontweight='bold')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    \n    # Training Accuracy\n    plt.subplot(2, 2, 3)\n    for idx, (model_name, history) in enumerate(histories.items()):\n        epochs = [i+1 for i in range(len(history.history['accuracy']))]\n        plt.plot(epochs, history.history['accuracy'], \n                color=colors[idx % len(colors)], linewidth=2, label=model_name)\n    plt.title('Training Accuracy Comparison', fontsize=16, fontweight='bold')\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    \n    # Validation Accuracy\n    plt.subplot(2, 2, 4)\n    for idx, (model_name, history) in enumerate(histories.items()):\n        epochs = [i+1 for i in range(len(history.history['val_accuracy']))]\n        val_acc = history.history['val_accuracy']\n        plt.plot(epochs, val_acc, \n                color=colors[idx % len(colors)], linewidth=2, label=model_name)\n        \n        # Mark best epoch\n        best_epoch = np.argmax(val_acc) + 1\n        best_acc = max(val_acc)\n        plt.scatter(best_epoch, best_acc, \n                   color=colors[idx % len(colors)], s=100, zorder=5)\n        \n    plt.title('Validation Accuracy Comparison', fontsize=16, fontweight='bold')\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n\ndef plot_comprehensive_results(results):\n    \"\"\"Comprehensive results visualization with 4-digit precision\"\"\"\n    fig, axes = plt.subplots(3, 3, figsize=(20, 18))\n    plt.style.use('default')\n    \n    models = list(results.keys())\n    colors = plt.cm.Set3(np.linspace(0, 1, len(models)))\n    \n    # Test Accuracy Comparison\n    test_accs = [results[model]['test_accuracy'] for model in models]\n    bars1 = axes[0, 0].bar(models, test_accs, color=colors)\n    axes[0, 0].set_title('Test Accuracy Comparison', fontsize=14, fontweight='bold')\n    axes[0, 0].set_ylabel('Accuracy')\n    axes[0, 0].set_ylim(0, 1)\n    axes[0, 0].tick_params(axis='x', rotation=45)\n    for i, v in enumerate(test_accs):\n        axes[0, 0].text(i, v + 0.01, f'{v:.4f}', ha='center', va='bottom', fontsize=9)\n    \n    # Parameters vs Accuracy\n    params = [results[model]['parameters'] / 1e6 for model in models]\n    scatter = axes[0, 1].scatter(params, test_accs, s=150, c=colors, alpha=0.7)\n    axes[0, 1].set_title('Parameters vs Test Accuracy', fontsize=14, fontweight='bold')\n    axes[0, 1].set_xlabel('Parameters (Millions)')\n    axes[0, 1].set_ylabel('Test Accuracy')\n    for i, model in enumerate(models):\n        axes[0, 1].annotate(model, (params[i], test_accs[i]), \n                           xytext=(5, 5), textcoords='offset points', fontsize=9)\n    \n    # Inference Time Comparison\n    inf_times = [results[model]['inference_time_per_image'] * 1000 for model in models]\n    bars2 = axes[0, 2].bar(models, inf_times, color=colors)\n    axes[0, 2].set_title('Inference Time per Image', fontsize=14, fontweight='bold')\n    axes[0, 2].set_ylabel('Time (ms)')\n    axes[0, 2].tick_params(axis='x', rotation=45)\n    for i, v in enumerate(inf_times):\n        axes[0, 2].text(i, v + max(inf_times)*0.02, f'{v:.4f}', ha='center', va='bottom', fontsize=9)\n    \n    # F1-Score Comparison\n    f1_scores = [results[model]['f1_macro'] for model in models]\n    bars3 = axes[1, 0].bar(models, f1_scores, color=colors)\n    axes[1, 0].set_title('F1-Score (Macro Average)', fontsize=14, fontweight='bold')\n    axes[1, 0].set_ylabel('F1-Score')\n    axes[1, 0].tick_params(axis='x', rotation=45)\n    for i, v in enumerate(f1_scores):\n        axes[1, 0].text(i, v + 0.01, f'{v:.4f}', ha='center', va='bottom', fontsize=9)\n    \n    # Mean Average Precision\n    mean_aps = [results[model]['mean_ap'] for model in models]\n    bars4 = axes[1, 1].bar(models, mean_aps, color=colors)\n    axes[1, 1].set_title('Mean Average Precision', fontsize=14, fontweight='bold')\n    axes[1, 1].set_ylabel('mAP')\n    axes[1, 1].tick_params(axis='x', rotation=45)\n    for i, v in enumerate(mean_aps):\n        axes[1, 1].text(i, v + 0.01, f'{v:.4f}', ha='center', va='bottom', fontsize=9)\n    \n    # Precision vs Recall\n    precisions = [results[model]['precision_macro'] for model in models]\n    recalls = [results[model]['recall_macro'] for model in models]\n    scatter = axes[1, 2].scatter(recalls, precisions, s=150, c=colors, alpha=0.7)\n    axes[1, 2].set_title('Precision vs Recall', fontsize=14, fontweight='bold')\n    axes[1, 2].set_xlabel('Recall (Macro)')\n    axes[1, 2].set_ylabel('Precision (Macro)')\n    for i, model in enumerate(models):\n        axes[1, 2].annotate(model, (recalls[i], precisions[i]), \n                           xytext=(5, 5), textcoords='offset points', fontsize=9)\n    \n    # Efficiency Score (Accuracy / Parameters)\n    efficiency = [results[model]['test_accuracy'] / (results[model]['parameters'] / 1e6) for model in models]\n    bars5 = axes[2, 0].bar(models, efficiency, color=colors)\n    axes[2, 0].set_title('Efficiency Score (Acc/Params)', fontsize=14, fontweight='bold')\n    axes[2, 0].set_ylabel('Efficiency Score')\n    axes[2, 0].tick_params(axis='x', rotation=45)\n    for i, v in enumerate(efficiency):\n        axes[2, 0].text(i, v + max(efficiency)*0.02, f'{v:.4f}', ha='center', va='bottom', fontsize=9)\n    \n    # Top-2 Accuracy\n    top2_accs = [results[model]['test_top2_accuracy'] for model in models]\n    bars6 = axes[2, 1].bar(models, top2_accs, color=colors)\n    axes[2, 1].set_title('Top-2 Accuracy', fontsize=14, fontweight='bold')\n    axes[2, 1].set_ylabel('Top-2 Accuracy')\n    axes[2, 1].tick_params(axis='x', rotation=45)\n    for i, v in enumerate(top2_accs):\n        axes[2, 1].text(i, v + 0.01, f'{v:.4f}', ha='center', va='bottom', fontsize=9)\n    \n    # Speed vs Accuracy Trade-off\n    axes[2, 2].scatter(inf_times, test_accs, s=150, c=colors, alpha=0.7)\n    axes[2, 2].set_title('Speed vs Accuracy Trade-off', fontsize=14, fontweight='bold')\n    axes[2, 2].set_xlabel('Inference Time (ms)')\n    axes[2, 2].set_ylabel('Test Accuracy')\n    for i, model in enumerate(models):\n        axes[2, 2].annotate(model, (inf_times[i], test_accs[i]), \n                           xytext=(5, 5), textcoords='offset points', fontsize=9)\n    \n    plt.tight_layout()\n    plt.show()\n\n# Main execution: Train models one by one, with Eye Disease Classifier at the end\nprint(\"\\n\" + \"=\"*80)\nprint(\"SEQUENTIAL MODEL TRAINING - EYE DISEASE CLASSIFICATION\")\nprint(\"=\"*80)\n\n# Define models in order (Eye Disease Classifier will be trained last)\nmodels_to_train = [\n    ('EfficientNet-B0', create_efficientnet),\n    ('MobileNet-V2', create_mobilenet_v2), \n    ('VGG16', create_vgg16),\n    ('DenseNet121', create_denseNet201)]\n\n# Store results\nresults = {}\nhistories = {}\ntrained_models = {}\n\n# Train each model sequentially\nfor model_name, model_creator in models_to_train:\n    print(f\"\\n{'='*60}\")\n    print(f\"CREATING AND TRAINING: {model_name}\")\n    print(f\"{'='*60}\")\n    \n    # Create the model\n    model = model_creator(num_classes)\n    \n    # Build the model using a real batch from train_gen if not built\n    if not model.built:\n        x_batch, y_batch = next(iter(train_gen))\n        _ = model(x_batch)\n    \n    print(f\"Parameters: {model.count_params():,}\")\n    \n    # Reset generators before training\n    train_gen.reset()\n    valid_gen.reset()\n    \n    # Train model\n    trained_model, history = compile_and_train_model(\n        model, model_name, train_gen, valid_gen, epochs=30\n    )\n    \n    # Store trained model and history\n    trained_models[model_name] = trained_model\n    histories[model_name] = history\n    \n    # Evaluate on test set\n    test_gen.reset()\n    test_loss, test_acc, test_top2_acc = trained_model.evaluate(test_gen, verbose=0)\n    \n    # Get predictions for confusion matrix and classification report\n    test_gen.reset()\n    y_pred_probs = trained_model.predict(test_gen, verbose=0)\n    y_pred_classes = np.argmax(y_pred_probs, axis=1)\n    \n    # Get true labels\n    y_true = test_gen.classes\n    \n    # Measure inference time\n    test_gen.reset()\n    avg_batch_time, avg_image_time = measure_inference_time(trained_model, test_gen)\n    \n    # Plot confusion matrix with 4-digit precision\n    cm = plot_confusion_matrix(y_true, y_pred_classes, class_names, model_name)\n    \n    # Generate classification report with 4-digit precision\n    class_report = generate_classification_report_4digits(\n        y_true, y_pred_classes, class_names, model_name\n    )\n    \n    # Plot GradCAM examples (for CNN-based models)\n    if any(keyword in model_name for keyword in ['Eye Disease', 'EfficientNet', 'MobileNet', 'VGG', 'DenseNet201', 'MobileNet_v2']):\n        plot_gradcam_examples(trained_model, test_gen, class_names, model_name)\n        \n    \n    # Calculate precision-recall curves\n    y_true_onehot = tf.keras.utils.to_categorical(y_true, num_classes)\n    \n    # Calculate mean AP with 4-digit precision\n    mean_ap = np.mean([average_precision_score(y_true_onehot[:, i], y_pred_probs[:, i]) \n                      for i in range(len(class_names))])\n    \n    # Store results with 4-digit precision\n    results[model_name] = {\n        'test_accuracy': round(test_acc, 4),\n        'test_top2_accuracy': round(test_top2_acc, 4),\n        'test_loss': round(test_loss, 4),\n        'parameters': model.count_params(),\n        'best_val_accuracy': round(max(history.history['val_accuracy']), 4),\n        'mean_ap': round(mean_ap, 4),\n        'precision_macro': round(class_report['macro avg']['precision'], 4),\n        'recall_macro': round(class_report['macro avg']['recall'], 4),\n        'f1_macro': round(class_report['macro avg']['f1-score'], 4),\n        'inference_time_per_image': round(avg_image_time, 6),\n        'inference_time_per_batch': round(avg_batch_time, 4)\n    }\n    \n    print(f\"\\n{'='*50}\")\n    print(f\"TEST RESULTS FOR {model_name}:\")\n    print(f\"{'='*50}\")\n    print(f\"Test Accuracy: {test_acc:.4f}\")\n    print(f\"Test Top-2 Accuracy: {test_top2_acc:.4f}\")\n    print(f\"Test Loss: {test_loss:.4f}\")\n    print(f\"Mean AP: {mean_ap:.4f}\")\n    print(f\"F1-Score (Macro): {class_report['macro avg']['f1-score']:.4f}\")\n    print(f\"Precision (Macro): {class_report['macro avg']['precision']:.4f}\")\n    print(f\"Recall (Macro): {class_report['macro avg']['recall']:.4f}\")\n    print(f\"Inference time per image: {avg_image_time*1000:.4f} ms\")\n    print(f\"Best Validation Accuracy: {max(history.history['val_accuracy']):.4f}\")\n    print(f\"Parameters: {model.count_params():,}\")\n    \n    # Clean up to save memory (optional)\n    if model_name != 'Eye Disease Classifier':  # Keep your custom model\n        del model\n\n# Combined visualizations after all models are trained\nprint(f\"\\n{'='*80}\")\nprint(\"COMBINED TRAINING HISTORIES VISUALIZATION\")\nprint(f\"{'='*80}\")\nplot_combined_training_histories(histories)\n\n# Comprehensive results visualization\nprint(f\"\\n{'='*80}\")\nprint(\"COMPREHENSIVE RESULTS VISUALIZATION\")\nprint(f\"{'='*80}\")\nplot_comprehensive_results(results)\n\n# Print final detailed comparison with 4-digit precision\nprint(\"\\n\" + \"=\"*130)\nprint(\"COMPREHENSIVE RESULTS COMPARISON (4-DIGIT PRECISION)\")\nprint(\"=\"*130)\n\nresults_df = pd.DataFrame(results).T\nresults_df = results_df.sort_values('test_accuracy', ascending=False)\n\nprint(f\"{'Model':<20} {'Test Acc':<10} {'Top-2 Acc':<10} {'mAP':<10} {'F1':<10} {'Precision':<10} {'Recall':<10} {'Params(M)':<12} {'Time(ms)':<12}\")\nprint(\"-\" * 130)\n\nfor model_name, row in results_df.iterrows():\n    print(f\"{model_name:<20} \"\n          f\"{row['test_accuracy']:<10.4f} \"\n          f\"{row['test_top2_accuracy']:<10.4f} \"\n          f\"{row['mean_ap']:<10.4f} \"\n          f\"{row['f1_macro']:<10.4f} \"\n          f\"{row['precision_macro']:<10.4f} \"\n          f\"{row['recall_macro']:<10.4f} \"\n          f\"{row['parameters']/1e6:<12.2f} \"\n          f\"{row['inference_time_per_image']*1000:<12.4f}\")\n\n# Final analysis and recommendations\nprint(\"\\n\" + \"=\"*130)\nprint(\"FINAL ANALYSIS AND RECOMMENDATIONS\")\nprint(\"=\"*130)\n\nbest_model = results_df.index[0]\nefficiency_scores = results_df['test_accuracy'] / (results_df['parameters'] / 1e6)\nmost_efficient = efficiency_scores.idxmax()\nfastest = results_df['inference_time_per_image'].idxmin()\n\nprint(f\"ðŸ† BEST OVERALL PERFORMANCE: {best_model}\")\nprint(f\"   â†’ Test Accuracy: {results_df.loc[best_model, 'test_accuracy']:.4f}\")\nprint(f\"   â†’ F1-Score: {results_df.loc[best_model, 'f1_macro']:.4f}\")\nprint(f\"   â†’ mAP: {results_df.loc[best_model, 'mean_ap']:.4f}\")\nprint(f\"   â†’ Parameters: {results_df.loc[best_model, 'parameters']/1e6:.2f}M\")\n\nprint(f\"\\nâš¡ MOST EFFICIENT: {most_efficient}\")\nprint(f\"   â†’ Efficiency Score: {efficiency_scores[most_efficient]:.4f}\")\nprint(f\"   â†’ Test Accuracy: {results_df.loc[most_efficient, 'test_accuracy']:.4f}\")\nprint(f\"   â†’ Parameters: {results_df.loc[most_efficient, 'parameters']/1e6:.2f}M\")\n\nprint(f\"\\nðŸš€ FASTEST INFERENCE: {fastest}\")\nprint(f\"   â†’ Inference Time: {results_df.loc[fastest, 'inference_time_per_image']*1000:.4f} ms\")\nprint(f\"   â†’ Test Accuracy: {results_df.loc[fastest, 'test_accuracy']:.4f}\")\nprint(f\"   â†’ FPS: {1/results_df.loc[fastest, 'inference_time_per_image']:.1f}\")\n\n# Eye Disease Classifier specific analysis\nif 'Eye Disease Classifier' in results_df.index:\n    eye_model_results = results_df.loc['Eye Disease Classifier']\n    print(f\"\\nðŸ”¬ YOUR EYE DISEASE CLASSIFIER ANALYSIS:\")\n    print(f\"   â†’ Test Accuracy: {eye_model_results['test_accuracy']:.4f}\")\n    print(f\"   â†’ F1-Score: {eye_model_results['f1_macro']:.4f}\")\n    print(f\"   â†’ Mean AP: {eye_model_results['mean_ap']:.4f}\")\n    print(f\"   â†’ Efficiency: {efficiency_scores['Eye Disease Classifier']:.4f}\")\n    print(f\"   â†’ Inference Time: {eye_model_results['inference_time_per_image']*1000:.4f} ms\")\n    print(f\"   â†’ Rank by Accuracy: {list(results_df.index).index('Eye Disease Classifier') + 1}/{len(results_df)}\")\n\n# Save comprehensive results\nresults_df.to_csv('eye_disease_comprehensive_results.csv')\nprint(f\"\\nðŸ“Š Results saved to: eye_disease_comprehensive_results.csv\")\nprint(f\"\\n{'='*130}\")\nprint(\"ANALYSIS COMPLETE - ALL MODELS TRAINED SEQUENTIALLY\")\nprint(f\"{'='*130}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T12:18:26.331508Z","iopub.execute_input":"2025-12-15T12:18:26.331788Z","iopub.status.idle":"2025-12-15T13:41:33.512967Z","shell.execute_reply.started":"2025-12-15T12:18:26.331765Z","shell.execute_reply":"2025-12-15T13:41:33.512357Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trained_models[\"MobileNet-V2\"].save(\"mobilenetv2_deployment_model.h5\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T13:58:38.807262Z","iopub.execute_input":"2025-12-15T13:58:38.807777Z","iopub.status.idle":"2025-12-15T13:58:39.282463Z","shell.execute_reply.started":"2025-12-15T13:58:38.807754Z","shell.execute_reply":"2025-12-15T13:58:39.281872Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\n\n# Get trained MobileNetV2 model\nmobilenet_model = trained_models['MobileNet-V2']\n\n# Convert to TFLite\nconverter = tf.lite.TFLiteConverter.from_keras_model(mobilenet_model)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\n\ntflite_model = converter.convert()\n\n# Save TFLite model\nwith open(\"mobilenetv2_eye_disease.tflite\", \"wb\") as f:\n    f.write(tflite_model)\n\nprint(\"âœ… MobileNetV2 TFLite model saved successfully\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T14:12:30.669001Z","iopub.execute_input":"2025-12-15T14:12:30.669729Z","iopub.status.idle":"2025-12-15T14:12:39.848681Z","shell.execute_reply.started":"2025-12-15T14:12:30.669703Z","shell.execute_reply":"2025-12-15T14:12:39.847880Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!ls -lh /kaggle/working/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T14:13:54.492571Z","iopub.execute_input":"2025-12-15T14:13:54.492862Z","iopub.status.idle":"2025-12-15T14:13:54.837544Z","shell.execute_reply.started":"2025-12-15T14:13:54.492841Z","shell.execute_reply":"2025-12-15T14:13:54.836584Z"}},"outputs":[],"execution_count":null}]}